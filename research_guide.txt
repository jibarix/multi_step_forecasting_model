# Validating a Multi-Step Vehicle Sales Forecasting Model

**Overview:** Research in forecasting and econometrics supports many elements of the proposed multi-step vehicle sales forecasting model. Key studies (both academic and governmental) provide evidence for dynamic variable selection techniques, two-stage (hierarchical) forecasting, handling mixed frequencies, ensuring forecast consistency across levels, and rigorous validation against benchmarks. Below, we summarize findings in each area and cite relevant sources.

## 1. Dynamic Anchor Variable Selection  
**Selecting predictors with Granger tests & information criteria:**  Empirical studies endorse using statistical tests and criteria to choose economic indicators that truly drive the forecast. Ashley, Granger, and Schmalensee (1980) famously advocated comparing out-of-sample forecast accuracy to test whether adding a candidate variable *actually* improves predictions ([Word Pro - 05clark](https://www.kansascityfed.org/documents/5423/pdf-RWP00-05.pdf#:~:text=of%20y%20that%20includes%20x,1983)). In practice, this means performing Granger-causality tests and then validating with metrics like RMSE on holdout samples. For example, Clark & McCracken (2000) show that if a variable **Granger-causes** the target, a model including it should beat a no-predictor benchmark in out-of-sample RMSE ([Word Pro - 05clark](https://www.kansascityfed.org/documents/5423/pdf-RWP00-05.pdf#:~:text=for%20y%2C%20forecasts%20from%20the,1983)). Such approaches help avoid overfitting and ensure chosen indicators have real predictive power, not just in-sample fit.

**AIC/BIC for indicator selection:**  Information criteria like AIC/BIC are widely used to select variables and lag structures in time-series models. Federal Reserve researchers note that traditional VAR models often rely on AIC/BIC to include only those lags/variables that improve model fit ([Granger-causal-priority and choice of variables in vector autoregressions](https://www.ecb.europa.eu/pub/pdf/scpwps/ecbwp1600.pdf#:~:text=belong%20in%20a%20quarterly%20VAR,VAR%20with%20real%20GDP%2C%20the)) ([Granger-causal-priority and choice of variables in vector autoregressions](https://www.ecb.europa.eu/pub/pdf/scpwps/ecbwp1600.pdf#:~:text=price%20level%2C%20and%20the%20short%02term,the%20price%20of%20oil)). By penalizing model complexity, these criteria help identify a parsimonious set of predictors that likely generalize well. In high-dimensional settings, recent work even combines info-criteria with Granger testing to determine an optimal variable subset ([Word Pro - 05clark](https://www.kansascityfed.org/documents/5423/pdf-RWP00-05.pdf#:~:text=Schmalensee%20,widely%20used%2C%20little%20is%20known)).

**Replacing GDP with alternative anchors:**  Several studies validate using other economic indicators (beyond GDP growth) as anchors for vehicle sales forecasts. A University of Michigan analysis found **consumer confidence, interest rates, and inflation** have strong relationships with auto demand ([Study:  economy, competition and auto dealer |  University of Michigan News](https://news.umich.edu/study-economy-competition-and-auto-dealer/#:~:text=how%20those%20trends%2C%20in%20turn%2C,starts%2C%20and%20personal%20consumption%20expenditures)). In fact, higher consumer sentiment correlates with surging car sales, whereas rising interest rates coincide with weaker sales ([Study:  economy, competition and auto dealer |  University of Michigan News](https://news.umich.edu/study-economy-competition-and-auto-dealer/#:~:text=Unemployment%20rates%20tend%20to%20affect,less%20tied%20to%20changes%20in)). This suggests that indices like the Consumer Confidence Index or Federal Funds Rate can serve as leading anchors in lieu of (or alongside) GDP. An industry study likewise observed a **tight link between consumer confidence and new car sales** with about a 2-month lead ([The Consumer Confidence Index can predict car sales](https://www.rekonpartners.hu/post/the-consumer-confidence-index-can-predict-car-sales#:~:text=clients%E2%80%99%20outstanding%20sales%20figures%20in,analysts%20and%20car%20traders%20alike)) ([The Consumer Confidence Index can predict car sales](https://www.rekonpartners.hu/post/the-consumer-confidence-index-can-predict-car-sales#:~:text=The%20research%20underscores%20that%20the,demand%20based%20on%20consumer%20sentiment)). Moreover, a SAS Global Forum paper on auto sales used dynamic variable selection to include factors such as unemployment, fuel prices, and consumer finance expectations (instead of just GDP). Their vector error-correction model with these **alternate drivers** achieved **60% lower RMSE** than a univariate model ([340-2012: Variable Selection for Multivariate Cointegrated Time Series Prediction with PROC VARCLUS in SAS® Enterprise Miner™ 7.1](https://support.sas.com/resources/papers/proceedings12/340-2012.pdf#:~:text=Model%20RMSE%20%2812,1%29%29%20and%201)) ([340-2012: Variable Selection for Multivariate Cointegrated Time Series Prediction with PROC VARCLUS in SAS® Enterprise Miner™ 7.1](https://support.sas.com/resources/papers/proceedings12/340-2012.pdf#:~:text=series%20models%20are%20selected%20to,2012%20Statistics%20and%20Data%20Analysis)). This confirms that using the **right anchor variable** (e.g. interest rates or sentiment when they are more predictive) can substantially improve forecast accuracy over a one-size-fits-all GDP anchor.

**Sources:** Dynamic variable selection is supported by forecasting methodology research ([Word Pro - 05clark](https://www.kansascityfed.org/documents/5423/pdf-RWP00-05.pdf#:~:text=of%20y%20that%20includes%20x,1983)) and demonstrated in automotive demand studies ([Study:  economy, competition and auto dealer |  University of Michigan News](https://news.umich.edu/study-economy-competition-and-auto-dealer/#:~:text=how%20those%20trends%2C%20in%20turn%2C,starts%2C%20and%20personal%20consumption%20expenditures)) ([340-2012: Variable Selection for Multivariate Cointegrated Time Series Prediction with PROC VARCLUS in SAS® Enterprise Miner™ 7.1](https://support.sas.com/resources/papers/proceedings12/340-2012.pdf#:~:text=Model%20RMSE%20%2812,1%29%29%20and%201)).

## 2. Two-Stage Forecasting Process  
**Intermediate forecasting via PCA (dimension reduction):**  High-dimensional forecasting methods show clear benefits to a two-stage approach: first compress many correlated economic series into a few summary factors, then predict the target using those factors. Stock and Watson’s **“diffusion index”** approach is a prime example. They applied principal component analysis (PCA) to 215 economic time series to create a handful of latent factors, and then used those factors to forecast various U.S. indicators ([Macroeconomic Forecasting Using Diffusion Indexes](https://www.princeton.edu/~mwatson/papers/Stock_Watson_JBES_2002.pdf#:~:text=This%20article%20studies%20forecasting%20a,215%20predictors%20in%20simulated%20real)). The result: **factor-based forecasts outperformed univariate ARIMA, small VARs, and even leading indicator models** over 1970–1998 ([Macroeconomic Forecasting Using Diffusion Indexes](https://www.princeton.edu/~mwatson/papers/Stock_Watson_JBES_2002.pdf#:~:text=6,a%20reasonable%20cost%2C%20thousands%20of)). In other words, condensing information via PCA yielded more accurate vehicle sales predictors than selecting any single macro series. This validates the practice of forecasting intermediate economic indices (e.g. a composite of rates, income, confidence) as an input to the final vehicle sales model.

**Effectiveness of hierarchical (multi-stage) modeling:**  Economic forecasting often occurs in a hierarchy (e.g. predict GDP or income first, then vehicle sales). Research on hierarchical forecasting shows that a structured two-stage approach can improve accuracy and coherence. For instance, Hyndman et al. (2011) propose forecasting all levels of a hierarchy separately and then reconciling them for consistency ([Optimal combination forecasts for hierarchical time series](https://ideas.repec.org/a/eee/csdana/v55y2011i9p2579-2589.html#:~:text=series,that%20our%20method%20performs%20well)). In their study of Australian tourism (an economic time series broken by region and purpose), this **optimal reconciliation** method outperformed traditional top-down or bottom-up forecasts ([Optimal combination forecasts for hierarchical time series](https://ideas.repec.org/a/eee/csdana/v55y2011i9p2579-2589.html#:~:text=series,that%20our%20method%20performs%20well)). The implication is that a two-stage process – forecasting a macroeconomic driver (or aggregate) and then the sectoral outcome – can leverage information at both levels. Even if one simply uses a **hierarchical forecast** (first an anchor like GDP or income, then vehicle sales conditional on that), it aligns with the principle that modeling the economic context first can yield better final forecasts. Academic evidence thus supports such staged forecasting: PCA or factor models handle the **dimensionality reduction** in stage 1 ([Macroeconomic Forecasting Using Diffusion Indexes](https://www.princeton.edu/~mwatson/papers/Stock_Watson_JBES_2002.pdf#:~:text=This%20article%20studies%20forecasting%20a,215%20predictors%20in%20simulated%20real)), and hierarchical methods ensure the stage-1 outlook and stage-2 forecasts are used in an integrated, optimal way ([Optimal combination forecasts for hierarchical time series](https://ideas.repec.org/a/eee/csdana/v55y2011i9p2579-2589.html#:~:text=series,that%20our%20method%20performs%20well)).

**Sources:** Two-stage approaches are validated by factor-model success in macro forecasting ([Macroeconomic Forecasting Using Diffusion Indexes](https://www.princeton.edu/~mwatson/papers/Stock_Watson_JBES_2002.pdf#:~:text=6,a%20reasonable%20cost%2C%20thousands%20of)) and by hierarchical forecasting research showing accuracy gains from multi-level modeling ([Optimal combination forecasts for hierarchical time series](https://ideas.repec.org/a/eee/csdana/v55y2011i9p2579-2589.html#:~:text=series,that%20our%20method%20performs%20well)).

## 3. Multi-Frequency Data Handling  
**MIDAS models for mixed frequencies:**  When forecasting with indicators sampled at different frequencies (e.g. monthly sales vs. quarterly GDP), Mixed Data Sampling (MIDAS) regression is a state-of-the-art solution. A body of academic work demonstrates that MIDAS improves forecast accuracy by incorporating high-frequency information. For example, Clements & Galvão (2008) and Marcellino & Schumacher (2010) used MIDAS to predict quarterly GDP from monthly indicators, achieving higher accuracy for the US and Germany ([](https://people.math.carleton.ca/~smills/2017-18/STAT4601-5703/Research%20Projects/2018%20Submissions/JankovicLamBilgin/Time%20Series.pdf#:~:text=also%20proven%20to%20be%20useful,data%20significantly%20improves%20forecast%20accuracy)). Andreou, Ghysels & Kourtellos (2013) further showed that including daily financial data via MIDAS significantly improves GDP growth forecasts ([](https://people.math.carleton.ca/~smills/2017-18/STAT4601-5703/Research%20Projects/2018%20Submissions/JankovicLamBilgin/Time%20Series.pdf#:~:text=United%20States%20and%20Germany%2C%20respectively,data%20significantly%20improves%20forecast%20accuracy)). A summary of these studies concludes that **using mixed-frequency data “significantly improves forecast accuracy.”** ([](https://people.math.carleton.ca/~smills/2017-18/STAT4601-5703/Research%20Projects/2018%20Submissions/JankovicLamBilgin/Time%20Series.pdf#:~:text=United%20States%20and%20Germany%2C%20respectively,data%20significantly%20improves%20forecast%20accuracy)). In short, MIDAS allows the model to exploit timely monthly data (like industrial production or consumer sentiment) to predict lower-frequency outcomes (like vehicle sales or GDP) better than ignoring those signals. Empirical tests find MIDAS especially effective at short horizons: one study comparing MIDAS to a mixed-frequency VAR found MIDAS performed best for nowcasts and 1-quarter-ahead GDP forecasts ([MIDAS versus mixed-frequency VAR: nowcasting GDP in the euro area](https://www.bundesbank.de/resource/blob/703478/e103f573e8ad3241e7cd65431dcbaf6c/mL/2009-03-20-dkp-07-data.pdf#:~:text=results%20show%20that%20forecasts%20with,information%20content%20up%20to%20one)). This evidence validates the model’s use of MIDAS or similar techniques for aligning monthly vehicle sales with quarterly economic drivers.

**Temporal disaggregation (Denton method):**  Government statistical agencies have long used the **Denton method** to align data of different frequencies while preserving patterns. Denton’s proportional adjustment approach is designed to distribute low-frequency totals across higher-frequency periods in a way that **minimizes distortion of the high-frequency movement**. A comprehensive U.S. Bureau of Economic Analysis study compared various temporal alignment methods (Denton variants, Chow-Lin regression, etc.) for national accounts. The clear result was that the **modified Denton first-difference method outperforms other techniques** in most cases ([An Empirical Comparison of Methods for Temporal Distribution and Interpolation at the National Accounts](https://www.bea.gov/index.php/system/files/papers/WP2007-4.pdf#:~:text=contain%20negative%20values,Monsell%20from%20the%20Census%20Bureau)). Denton’s method best preserved short-term monthly/quarterly variability while matching annual benchmarks, making it superior in terms of resulting accuracy ([An Empirical Comparison of Methods for Temporal Distribution and Interpolation at the National Accounts](https://www.bea.gov/index.php/system/files/papers/WP2007-4.pdf#:~:text=contain%20negative%20values,Monsell%20from%20the%20Census%20Bureau)). Likewise, Eurostat’s official guidelines acknowledge Denton as a simple and effective way to impose consistency between annual and sub-annual data ([Temporal disaggregation of economic time series: towards a dynamic extension](https://ec.europa.eu/eurostat/documents/3888793/5816173/KS_AN-03-035-EN.PDF/21c4417c-dbec-45ec-b440-fe8bf95661b7?version=1.0#:~:text=As%20suggested%20by%20Proietti%20,%2C%20is)). By using Denton or related alignment, the forecast model can incorporate, say, quarterly GDP or yearly income into monthly vehicle sales predictions without introducing large distortions. This approach has been **validated in government practice** – for example, Statistics Canada and Eurostat routinely use Denton adjustments for temporally disaggregating economic series ([An Empirical Comparison of Methods for Temporal Distribution and Interpolation at the National Accounts](https://www.bea.gov/index.php/system/files/papers/WP2007-4.pdf#:~:text=temporal%20and%20contemporaneous%20constraints%3B%20and,)) ([An Empirical Comparison of Methods for Temporal Distribution and Interpolation at the National Accounts](https://www.bea.gov/index.php/system/files/papers/WP2007-4.pdf#:~:text=modified%20Denton%20proportional%20first%20difference,Monsell%20from%20the%20Census%20Bureau)). In forecasting, combining MIDAS for predictive modeling and Denton for reconciling frequencies gives a robust toolkit for multi-frequency data. 

**Sources:** Mixed-frequency modeling is supported by numerous studies showing MIDAS gains ([](https://people.math.carleton.ca/~smills/2017-18/STAT4601-5703/Research%20Projects/2018%20Submissions/JankovicLamBilgin/Time%20Series.pdf#:~:text=and%20Galv%CB%9Cao%20,data%20significantly%20improves%20forecast%20accuracy)) ([MIDAS versus mixed-frequency VAR: nowcasting GDP in the euro area](https://www.bundesbank.de/resource/blob/703478/e103f573e8ad3241e7cd65431dcbaf6c/mL/2009-03-20-dkp-07-data.pdf#:~:text=results%20show%20that%20forecasts%20with,information%20content%20up%20to%20one)). Government research confirms Denton’s method as a best practice for temporal disaggregation ([An Empirical Comparison of Methods for Temporal Distribution and Interpolation at the National Accounts](https://www.bea.gov/index.php/system/files/papers/WP2007-4.pdf#:~:text=contain%20negative%20values,Monsell%20from%20the%20Census%20Bureau)), ensuring monthly and quarterly data are consistently integrated.

## 4. Hierarchical Consistency in Forecasting  
**Bottom-up vs. top-down vs. optimal reconciliation:**  Ensuring forecasts are coherent across aggregation levels is crucial in multi-level modeling (e.g. total market vs segments). Academic research has thoroughly evaluated bottom-up and top-down approaches, and finds that **neither is uniformly best – instead, an optimal combination often excels**. Hyndman et al. (2011) introduced an **optimal reconciliation** method that produces forecasts which **add up correctly across the hierarchy and have minimum variance** ([Optimal combination forecasts for hierarchical time series](https://ideas.repec.org/a/eee/csdana/v55y2011i9p2579-2589.html#:~:text=series,that%20our%20method%20performs%20well)) ([Optimal combination forecasts for hierarchical time series](https://ideas.repec.org/a/eee/csdana/v55y2011i9p2579-2589.html#:~:text=which%20provides%20optimal%20forecasts%20that,demand%20where%20the%20data%20are)). By forecasting each level independently and then combining them via a regression-based adjustment, they obtained more accurate predictions than standard bottom-up or top-down methods ([Optimal combination forecasts for hierarchical time series](https://ideas.repec.org/a/eee/csdana/v55y2011i9p2579-2589.html#:~:text=series,that%20our%20method%20performs%20well)). The reconciled forecasts were unbiased and respected aggregation constraints (e.g. sum of segments = total) by construction ([Optimal combination forecasts for hierarchical time series](https://ideas.repec.org/a/eee/csdana/v55y2011i9p2579-2589.html#:~:text=forecasting%20all%20series%20at%20all,that%20our%20method%20performs%20well)). In a simulation and case study, this optimal combination outperformed pure top-down allocation and pure bottom-up aggregation in terms of forecast error ([Optimal combination forecasts for hierarchical time series](https://ideas.repec.org/a/eee/csdana/v55y2011i9p2579-2589.html#:~:text=forecasting%20all%20series%20at%20all,of%20travel%20and%20geographical%20region)). 

**Empirical validation of hierarchical methods:**  Recent studies confirm the benefit of ensuring hierarchical consistency. Mohamed (2023) compared forecasting strategies for Egypt’s trade data (a hierarchical series: total trade, exports, imports). She found that **bottom-up forecasts and MinT (minimum trace variance optimal reconciliation) yielded the best accuracy** among individual methods ([Enhancing forecast accuracy using combination methods for the hierarchical time series approach - PubMed](https://pubmed.ncbi.nlm.nih.gov/37459348/#:~:text=different%20models%20can%20improve%20forecast,with%20the%20ARIMA%20model%20have)). Furthermore, combining forecasts from bottom-up and MinT improved accuracy even more, outperforming other approaches at both the aggregate and disaggregate levels ([Enhancing forecast accuracy using combination methods for the hierarchical time series approach - PubMed](https://pubmed.ncbi.nlm.nih.gov/37459348/#:~:text=forecasting%20have%20been%20considered%3B%20a,total)) ([Enhancing forecast accuracy using combination methods for the hierarchical time series approach - PubMed](https://pubmed.ncbi.nlm.nih.gov/37459348/#:~:text=exports%2C%20and%20total%20imports%29,to%20be%20a%20guide%20for)). This reinforces that **blended or optimally reconciled forecasts** can leverage the strengths of both bottom-up and top-down information. In practice, many government agencies also enforce hierarchical consistency; for example, national statistical offices reconcile regional forecasts to national totals (or vice versa) as recommended by the ESS guidelines ([[PDF] ESS guidelines on temporal disaggregation, benchmarking and ...](https://ec.europa.eu/eurostat/documents/3859598/9441376/KS-06-18-355-EN.pdf#:~:text=,in%20sets%20of%20time%20series)). The literature’s consensus is that **ensuring additivity and using combination methods leads to more robust and consistent forecasts** than arbitrarily choosing bottom-up or top-down. For the vehicle sales model, this means making sure forecasts of sub-categories (say by vehicle type or region) align with the overall sales forecast, using optimal combination if necessary to adjust any discrepancies.

**Sources:** Optimal reconciliation methods have been shown to outperform simple bottom-up or top-down forecasts ([Optimal combination forecasts for hierarchical time series](https://ideas.repec.org/a/eee/csdana/v55y2011i9p2579-2589.html#:~:text=series,that%20our%20method%20performs%20well)). For instance, combining hierarchical forecasts (MinT, etc.) improved accuracy in trade data forecasting ([Enhancing forecast accuracy using combination methods for the hierarchical time series approach - PubMed](https://pubmed.ncbi.nlm.nih.gov/37459348/#:~:text=different%20models%20can%20improve%20forecast,with%20the%20ARIMA%20model%20have)) ([Enhancing forecast accuracy using combination methods for the hierarchical time series approach - PubMed](https://pubmed.ncbi.nlm.nih.gov/37459348/#:~:text=good%20predictive%20performance%20than%20other,forecasting%20values%20of%20imports%20and)). These studies validate maintaining hierarchical consistency in multi-level forecasts.

## 5. Evaluation and Validation Techniques  
**Time-series cross-validation:**  Standard cross-validation must be adapted for time-series data, and research encourages using **rolling-origin evaluation** to assess forecast models. In time-series cross-validation (also called rolling window or rolling origin), one simulates multiple forecast scenarios by moving the training/validation split through time. Rob Hyndman demonstrates this procedure for multi-step forecasting: use the first $k$ observations to forecast the next $h$ steps, record the error, then roll forward and repeat ([Rob Hyndman on Time-Series Cross-Validation - The Business Forecasting Deal](https://blogs.sas.com/content/forecasting/2016/06/09/rob-hyndman-on-time-series-cross-validation/#:~:text=Hyndman%20shows%20how%20the%20time,multistep%20errors%20to%20be%20used)) ([Rob Hyndman on Time-Series Cross-Validation - The Business Forecasting Deal](https://blogs.sas.com/content/forecasting/2016/06/09/rob-hyndman-on-time-series-cross-validation/#:~:text=1,forecast%20in%20the%20previous%20blog)). This effectively creates a series of “out-of-sample” forecasts across the historical period. Hyndman notes that time-series CV provides a more reliable indication of model performance than a single train-test split, especially when data is limited ([Rob Hyndman on Time-Series Cross-Validation - The Business Forecasting Deal](https://blogs.sas.com/content/forecasting/2016/06/09/rob-hyndman-on-time-series-cross-validation/#:~:text=Image%3A%20Training%20data%20and%20Test,cover%20the%20desired%20forecasting%20horizon)) ([Rob Hyndman on Time-Series Cross-Validation - The Business Forecasting Deal](https://blogs.sas.com/content/forecasting/2016/06/09/rob-hyndman-on-time-series-cross-validation/#:~:text=Hyndman%20shows%20how%20the%20time,multistep%20errors%20to%20be%20used)). Academic examinations (e.g. Bergmeir et al. 2018) likewise find that rolling-origin cross-validation helps in selecting models and avoiding overfit in time series contexts. In practical terms, using techniques like **blocked cross-validation or expanding-window evaluation** ensures the vehicle sales model is tested on multiple folds of unseen data (while respecting temporal order) ([Rob Hyndman on Time-Series Cross-Validation - The Business Forecasting Deal](https://blogs.sas.com/content/forecasting/2016/06/09/rob-hyndman-on-time-series-cross-validation/#:~:text=1,forecast%20in%20the%20previous%20blog)). This rigorous approach is **widely recommended in forecasting literature** for validating model robustness.

**Benchmarking against naive models:**  Robustness is also measured by comparing a model’s accuracy to simple naive forecasts. Many studies stress that a new model should at least beat a naive baseline (such as “next period = last period” or a random walk). In macroeconomic forecasting, this has been a high bar – notable research has found that complex models often fail to outperform naive projections. For example, **Atkeson & Ohanian (2001) showed that no Phillips-curve-based model beat a naive forecast of constant inflation** for the next year ([Are Phillips Curves Useful for Forecasting Inflation? | Federal Reserve Bank of Minneapolis](https://www.minneapolisfed.org/research/quarterly-review/are-phillips-curves-useful-for-forecasting-inflation#:~:text=This%20study%20evaluates%20the%20conventional,the%20likelihood%20of%20accurately%20predicting)) ([Are Phillips Curves Useful for Forecasting Inflation? | Federal Reserve Bank of Minneapolis](https://www.minneapolisfed.org/research/quarterly-review/are-phillips-curves-useful-for-forecasting-inflation#:~:text=NAIRU%20models%20to%20the%20naive,in%20the%20inflation%20rate%20from)). Their evaluation over 15 years revealed that assuming “inflation next year = inflation this year” outperformed Fed and textbook models, highlighting the importance of benchmark tests. Similarly, Meese & Rogoff (1983) famously found exchange rate models could not beat a random walk (naive) forecast ([Word Pro - 05clark](https://www.kansascityfed.org/documents/5423/pdf-RWP00-05.pdf#:~:text=,1983)). Because of such findings, it’s become standard to include benchmarks like **no-change, historical average, or seasonal naïve forecasts** when evaluating new models. Forecast competitions (M3, M4, etc.) also reinforce this by using naive methods as yardsticks – often, only the best models significantly outperform these simple rules. In the context of vehicle sales, a naïve benchmark might be assuming sales grow at their long-term average or repeat last year’s monthly pattern. To claim validity, the multi-step model should demonstrate lower error (e.g. RMSE, MAE) than these naive alternatives. This practice is supported by both academic guidance and government forecasters’ accountability standards. In short, **achieving improvements over naive forecasts in out-of-sample tests** is a critical validation step for any economic forecasting model ([Are Phillips Curves Useful for Forecasting Inflation? | Federal Reserve Bank of Minneapolis](https://www.minneapolisfed.org/research/quarterly-review/are-phillips-curves-useful-for-forecasting-inflation#:~:text=This%20study%20evaluates%20the%20conventional,in%20the%20inflation%20rate%20from)).

**Sources:** Time-series cross-validation techniques (like rolling-origin) are recommended to reliably evaluate forecast models ([Rob Hyndman on Time-Series Cross-Validation - The Business Forecasting Deal](https://blogs.sas.com/content/forecasting/2016/06/09/rob-hyndman-on-time-series-cross-validation/#:~:text=Hyndman%20shows%20how%20the%20time,multistep%20errors%20to%20be%20used)) ([Rob Hyndman on Time-Series Cross-Validation - The Business Forecasting Deal](https://blogs.sas.com/content/forecasting/2016/06/09/rob-hyndman-on-time-series-cross-validation/#:~:text=1,forecast%20in%20the%20previous%20blog)). Moreover, studies have emphasized using naive forecasting as a benchmark – e.g., a simple year-ahead naive inflation forecast proved hard to beat by more complex models ([Are Phillips Curves Useful for Forecasting Inflation? | Federal Reserve Bank of Minneapolis](https://www.minneapolisfed.org/research/quarterly-review/are-phillips-curves-useful-for-forecasting-inflation#:~:text=NAIRU%20models%20to%20the%20naive,in%20the%20inflation%20rate%20from)). Consistently outperforming such benchmarks gives confidence in a model’s robustness.

